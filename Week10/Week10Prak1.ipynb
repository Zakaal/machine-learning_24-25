{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMRAyA3BLm57+0RytrHoxO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zakaal/machine-learning_24-25/blob/main/Week10/Week10Prak1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JOBSHEET 11**\n",
        "\n",
        "## **Praktikum 2: Klasifikasi Multi-label dengan Data CIFAT**"
      ],
      "metadata": {
        "id": "_T0EK1MhRUa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Nama : Mochammad Zakaro Al fajri**\n",
        "\n",
        "**Kelas : TI - 3F**\n",
        "\n",
        "**NIM : 2241720175**\n",
        "\n",
        "**No.absen : 14**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RUDp2mpKRcV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pby6UJeT9lAz",
        "outputId": "a542c845-bf2e-4b1d-9988-e6ba18901313"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PhGEwgqb9UXd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/machine-leaning/pertemuan-11/dataset/dataset/training_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ],
      "metadata": {
        "id": "09d4hFD59kY9",
        "outputId": "ea50d977-c207-47b0-ab6d-b10e9bdbc8d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/machine-leaning/pertemuan-11/dataset/dataset/training_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPxYjkovt2IV",
        "outputId": "a02f2bf4-02b7-4a0d-b5a3-b3239625f6be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "dUkvYQGFtdRO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "metadata": {
        "id": "gZnubgEwtgQB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "bxeSJzwFtiLp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "m_J0L3ygtk42"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "Y75E-nPMtmLG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "metadata": {
        "id": "YwtZvn-atq7s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "x-NdnxYPts94"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "25OEkr_-tuxB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUr_9YQEtwCX",
        "outputId": "53027dc8-7a0a-41fd-abf5-f06081d89a05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 962s 4s/step - loss: 0.6667 - accuracy: 0.5909 - val_loss: 0.6109 - val_accuracy: 0.6658\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 64s 257ms/step - loss: 0.5901 - accuracy: 0.6873 - val_loss: 0.5523 - val_accuracy: 0.7092\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 64s 254ms/step - loss: 0.5587 - accuracy: 0.7064 - val_loss: 0.5086 - val_accuracy: 0.7480\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 64s 257ms/step - loss: 0.5295 - accuracy: 0.7297 - val_loss: 0.4842 - val_accuracy: 0.7625\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 64s 258ms/step - loss: 0.4965 - accuracy: 0.7550 - val_loss: 0.4613 - val_accuracy: 0.7810\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 63s 254ms/step - loss: 0.4770 - accuracy: 0.7689 - val_loss: 0.5438 - val_accuracy: 0.7408\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 64s 256ms/step - loss: 0.4616 - accuracy: 0.7810 - val_loss: 0.4159 - val_accuracy: 0.8045\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.4465 - accuracy: 0.7890 - val_loss: 0.4074 - val_accuracy: 0.8138\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 64s 256ms/step - loss: 0.4485 - accuracy: 0.7918 - val_loss: 0.4250 - val_accuracy: 0.7980\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.4280 - accuracy: 0.8026 - val_loss: 0.3873 - val_accuracy: 0.8253\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 64s 258ms/step - loss: 0.4157 - accuracy: 0.8101 - val_loss: 0.3879 - val_accuracy: 0.8215\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.3920 - accuracy: 0.8196 - val_loss: 0.3643 - val_accuracy: 0.8338\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.3848 - accuracy: 0.8219 - val_loss: 0.3373 - val_accuracy: 0.8493\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 63s 252ms/step - loss: 0.3692 - accuracy: 0.8349 - val_loss: 0.3364 - val_accuracy: 0.8526\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 64s 255ms/step - loss: 0.3614 - accuracy: 0.8338 - val_loss: 0.3155 - val_accuracy: 0.8630\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 62s 250ms/step - loss: 0.3493 - accuracy: 0.8410 - val_loss: 0.2711 - val_accuracy: 0.8915\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 64s 255ms/step - loss: 0.3393 - accuracy: 0.8494 - val_loss: 0.2826 - val_accuracy: 0.8769\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 65s 258ms/step - loss: 0.3241 - accuracy: 0.8597 - val_loss: 0.2739 - val_accuracy: 0.8855\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 64s 258ms/step - loss: 0.3091 - accuracy: 0.8686 - val_loss: 0.3162 - val_accuracy: 0.8654\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 64s 255ms/step - loss: 0.2977 - accuracy: 0.8698 - val_loss: 0.2646 - val_accuracy: 0.8870\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 63s 254ms/step - loss: 0.2870 - accuracy: 0.8775 - val_loss: 0.2160 - val_accuracy: 0.9100\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 63s 254ms/step - loss: 0.2713 - accuracy: 0.8861 - val_loss: 0.2068 - val_accuracy: 0.9168\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 63s 251ms/step - loss: 0.2564 - accuracy: 0.8876 - val_loss: 0.1873 - val_accuracy: 0.9276\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.2474 - accuracy: 0.8970 - val_loss: 0.1739 - val_accuracy: 0.9305\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.2409 - accuracy: 0.9009 - val_loss: 0.1799 - val_accuracy: 0.9265\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b192c08a5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# Load and preprocess the image\n",
        "test_image = image.load_img('/content/drive/MyDrive/machine-leaning/pertemuan-11/dataset/dataset/single_prediction/cat_or_dog_1.jpg', target_size=(64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Predict the result\n",
        "result = cnn.predict(test_image)\n",
        "training_set_class_indices = {'cat': 0, 'dog': 1}  # Adjust this dictionary according to your training set classes\n",
        "\n",
        "# Interpret the result\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "\n",
        "print(f'The prediction is: {prediction}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPr2oCPaABL1",
        "outputId": "1fe827c6-949c-487d-b869-c8c86fdf79d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "The prediction is: dog\n"
          ]
        }
      ]
    }
  ]
}